LOG PROGETTO:


0 - INSTALLLAZIONE SOFTWARE VARI:

sudo apt install snapd
sudo snap install snapd
[installazione docker come da docker-commands.txt e sudo systemctl start docker per farlo partire al boot]
sudo snap install kubectl --classic #installa kubectl


1- INSTALLAZIONE KIND: serve per creare cluster con container docker

curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-arm64
sudo rm /usr/local/bin/kind # per sicurezza
chmod +x ./kind
sudo mv ./kind /usr/local/bin/kind

Per creare/cancellare il cluster Kind:
creazione file yaml kind-config.yaml
sudo kind create cluster --config kind-config.yaml
sudo kind delete cluster #rimuove tutti i nodi del cluster

comandi kind:
sudo kind get nodes
sudo kind get clusters
> sudo kind export kubeconfig --name kind #se kubectl non punta al cluster kind

comandi kubectl:
sudo kubectl config get-contexts #per vedere se punta a kind
sudo kubectl config use-context kind-kind #per usare il giusto contesto
sudo kubectl cluster-info #fa vedere il cluster
sudo kubectl cluster-info --context kind-kind #fa vedere il cluster
sudo kubectl cluster-info dump #dumpa un macello di info
sudo kubectl get nodes #vedi i nodi, se ce li hai. Con kind sono container e non nodi
sudo kubectl get all
Quindi posso usare tutti i comandi di microk8s kubectl senza scrivere microk8s!!!!


2 - CREAZIONE IMMAGINE DOCKER

sudo docker build -t giacomoalfani/asdn_project . #creazione immagine con nginx e index.html
sudo docker push giacomoalfani/asdn_project:latest #push su dockerhub
sudo docker pull giacomoalfani/asdn_project:latest #è meglio ma nel deploy c'è già il pull da docker hub


3 - FILE YAML E DEPLOY

creato file yaml di cluster deploy
> sudo kubectl apply -f cluster_deployment.yaml #deployamo la nostra appl nel cluster
sudo kubectl get all #si vedono

In teoria nel cluster composto da container che simulano i nodi, il deploy ha creato pod dentro i container (perchè per lui sono nodi)
Si verifica con sudo kubectl get pods -o wide che mostra se sono in esecuzione e su quale nodo girano.
Per esporre il cluster non serve creare manualmente il SERVICE ma lo fa lui così:

> sudo kubectl expose deployment <DEPLOY_NAME> --type=NodePort --port=80 #in DEPLOY_NAME per noi è clusterdeplo

(sudo kubectl get pods -o wide)
sudo kubectl get nodes -o wide #per vedere gli ip dei pod, quello del control-plane è quello dell'applicazione
per la porta fare sudo kubectl get services -A, ed è quella a destra del servizio chiamato clusterdeplo

Quindi per raggiungere il nodo bisogna fare:
curl <ip interno del nodo>:<porta esposta dal service all'esterno>
(curl 172.18.0.3:32350), questo permette di avere quella porta aperta nei nodi/container ma non lo collega con localhost ma non ci 
interessa per ora.


 APPLICAZIONE: http://172.18.0.2:30668/ #Giacomo
 JENKINS: http://172.18.0.3:32000/ #Giacomo


Quindi faremo curl 172.18.0.4:32350 per contattare la nostra applicazione
sudo kubectl exec -it clusterdeplo-7c6bcfb8c6-6wcc2 -- /bin/sh #per entrare nel nodo

modificando il file di config si sono aggiunte le configurazioni delle porte esposte.


4 - JENKINS (obsoleto in parte, vedi 4-BIS)

Jenkins dovrà:
    - collegarsi a github per verificare le nuove push;
    - quando c'è fare dei test sul codice, cioè su index.html, tipo di sintassi;
    - lanciare docker build sul dockerfile;
    - fare push immagine su dockerhub;
    - lanciare cluster_deployment.

creato file Jenkins_deployment.yaml
sudo kubectl apply -f jenkins_deployment.yaml

creato file jenkins_service.yaml
sudo kubectl apply -f jenkins_service.yaml

ha creato un nuovo pod dentro in nodo kind-worker, ecco perchè:
    - Separazione delle Responsabilità: Il nodo master è responsabile della gestione del cluster stesso, 
    incluso il controllo degli stati dei nodi, il monitoraggio e la gestione dei pod. Eseguire Jenkins 
    sul nodo master potrebbe mettere a rischio la stabilità del cluster, soprattutto se Jenkins richiede molte risorse.

    - Scalabilità e Manutenibilità: Separare Jenkins dal nodo master consente di scalare e gestire le risorse in modo 
    più efficiente. I nodi worker possono essere scalati in base alle necessità di carico di lavoro, mentre il master 
    rimane concentrato sulla gestione del cluster.

    - Sicurezza: Limitare i processi in esecuzione sul nodo master può ridurre la superficie di attacco, mantenendo 
    il master dedicato alla gestione del cluster.


JENKINS: 172.18.0.2:30000 (è 32000 ora)

sudo kubectl get pods -o wide #per nome pod jenkins
sudo kubectl logs <POD NAME> #per trovare la password
password Giacomo: fe8ca0c11dee4f9d924084c12c2987fe

credenziali:
    user: admin
    password: admin
    email: giacomoalfani@gmail.com

inserimento credenziali da manage credentials.
per kubernetes si crea un servizio, un file yaml e si applica:

sudo kubectl create serviceaccount jenkins-sa
sudo kubectl apply -f jenkins_sa_binding.yaml

per credenziali kubernetes:
sudo kubectl get secret -A #per capire quale è il nome del secret
il secret name è bootstrap-token-abcdef
sudo kubectl describe secret <SECRET NAME> -n kube-system #volendo si può togliere il nome del namespace ma non funziona
sudo kubectl create token jenkins-sa #se non funzionano i precedenti
token Giacomo:

eyJhbGciOiJSUzI1NiIsImtpZCI6IkliaDBycllaeTRvczZmcTdSWDlSa242MnJfNVRZNHU3bGNRaS00Um10SVEifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJl
cm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNzI1NDUxNTI0LCJpYXQiOjE3MjU0NDc5MjQsImlzcyI6Imh0dHBzOi8va3ViZXJ
uZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJkZWZhdWx0Iiwic2VydmljZWFjY291bnQiOn
sibmFtZSI6ImplbmtpbnMtc2EiLCJ1aWQiOiJjMmZkZjBjNC05MjU1LTQ1ZDItOWVhNC02NGExZTE2MTg0OGIifX0sIm5iZiI6MTcyNTQ0NzkyNCwic3ViI
joic3lzdGVtOnNlcnZpY2VhY2NvdW50OmRlZmF1bHQ6amVua2lucy1zYSJ9.TPYyKEH7dJ92RjdvkT_3V5zf3GYD41RWMVv_rFY-4Q_LhLXn_fSpjS4Xqo_
jX8Rbx9F04eiXkjoYL8X-7fxcm99K5aZSQgz-CYUWqhc6z-cc9dSHvAWXdUAOUFvEvxMg6Pl4jLUOH1Z05Iqzolb4o_dzmChiVAfC6hcgjhCvFsAmVp0UG_
PAvHINxADMlUH3VgfCvizwgqli7M3iksaIuR0O0hprCiA2PASvydjqhrUvOPjq0BaZCQYhwXYVXHGfKCEwNitQGeUPZsXiugtD6oS3MyCi3lHf9RON6kgcy
0Qe7Zs7EJkX-hsdA-ERU3V4V4D73IjxjwERrmU9Clksgw

Per creare le credenziali:
manage jenkins > credentials (sta dentro security) > system > click su global credentials > click su add credentials


4-BIS:

una volta fatto il setup del cluster abbiamo seguito la configurazione.txt dentro la cartella jenkins.
Uno dei problemi principali era quelli che kubectl si installava in una cartella sbagliata e jenkins non lo trovava.
Comunque era necessario crea l'immagine con già dentro installato almeno sudo altrimenti non si riusciva ad accedere 
ai privilegi di root per installare altro. Kubectl l'abbiamo installato con curl.

Fatto questo abbiamo cominciato a vedere i test che funzionavamo, quelli di kubectl.
Per il collegamento a github è necessario settare il job, creare un webhook su github, che informa il job di triggerarsi 
dietro la nostra push, ma si deve creare anche un modo grazie al quale la nostra macchina sia raggiungibile da github.
proviamo con ngrok.

troubleshooting:
sudo kubectl exec -it jenkins-55c5796998-6wfvf -- curl http://localhost:8080 #vedo se funziona jenkins da dentro il container

L'iimagine di jenkins su dockerhub non aveva nulla dentro, quindi si è creato un dockerfile apposito per installarci tutto, 
poi pushato su dockerhub e rideployato col file yaml di deploy di jenkins

5 - OPERATE

Ansible:

sudo apt install ansible #installare ansible
creata cartella ansible con file, specialmente il /.kube/config con sudo kind get kubeconfig | sudo tee ./.kube/config 1> /dev/null
modificato poi il yaml per funzionare senza bisogno di SSH
sudo apt install python3-kubernetes #per far funzionare il tutto

lanciare poi il comando assicurandosi che sia fatto il docker login

6 - GRAFANA E PROMETHEUS

# aggiunta repository (sudo importante sennò sembra che lo fa ma invece non lo fa!)
sudo helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
sudo helm repo add grafana https://grafana.github.io/helm-charts
sudo helm repo update

# creazione namespace apposito
sudo kubectl create namespace monitoring

# installazione prometheus
helm install prometheus prometheus-community/prometheus --namespace monitoring


# dopo il precedente comando dovrebbe apparire una cosa del genere:

NAME: prometheus
LAST DEPLOYED: Sat Sep 14 10:47:09 2024
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
The Prometheus server can be accessed via port 80 on the following DNS name from within your cluster:
prometheus-server.monitoring.svc.cluster.local

Get the Prometheus server URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace monitoring -l "app.kubernetes.io/name=prometheus,app.kubernetes.io/instance=prometheus" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace monitoring port-forward $POD_NAME 9090

The Prometheus alertmanager can be accessed via port 9093 on the following DNS name from within your cluster:
prometheus-alertmanager.monitoring.svc.cluster.local

Get the Alertmanager URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace monitoring -l "app.kubernetes.io/name=alertmanager,app.kubernetes.io/instance=prometheus" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace monitoring port-forward $POD_NAME 9093
#################################################################################
######   WARNING: Pod Security Policy has been disabled by default since    #####
######            it deprecated after k8s 1.25+. use                        #####
######            (index .Values "prometheus-node-exporter" "rbac"          #####
###### .          "pspEnabled") with (index .Values                         #####
######            "prometheus-node-exporter" "rbac" "pspAnnotations")       #####
######            in case you still need it.                                #####
#################################################################################

The Prometheus PushGateway can be accessed via port 9091 on the following DNS name from within your cluster:
prometheus-prometheus-pushgateway.monitoring.svc.cluster.local

Get the PushGateway URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace monitoring -l "app=prometheus-pushgateway,component=pushgateway" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace monitoring port-forward $POD_NAME 9091

For more information on running Prometheus, visit:
https://prometheus.io/

# fine stampa di installazione Prometheus

# installazione grafana
sudo helm install grafana grafana/grafana --namespace monitoring

# port forwarding